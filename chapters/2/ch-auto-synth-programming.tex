\chapter{Automatic Synthesizer Programming}
\label{chapter:asp-background}
The field of automatic synthesizer programming (ASP) emerged from the challenges with programming synthesizers and the desire for more intuitive answers for the question "How do I create \rule{1cm}{0.15mm} sound using \rule{1cm}{0.15mm} synthesizer?". James Justice \cite{justice1979analytic} was one of the first to try to answer this question in the late 1970s. Justice's work used analytic methods to estimate the parameters for the FM algorithm. This was an example of inverse synthesis, or sound matching, where a system estimates synthesizer parameters to replicate a target sound as closely as possible. Since then a large volume of work on inverse synthesis has been published, exploring a variety of synthesis techniques and algorithmic methods. In addition to inverse synthesis a number of other human interaction paradigms for automatic synthesizer programming have been explored including the use of semantic searches and intuitive \textit{macro} parameters. 

This chapter provides a brief overview and survey of the field of ASP with an emphasis on inverse synthesis. To situate ASP in a larger body of research, background and related work is provided.

% - other early methods focuses on analytic methods for programming synthesizers \cite{beauchamp1982synthesis, payne1987microcomputer, delprat1990parameter}

% % Does this is fit in on the topic of synthesizer timbre?? Or perhaps there is an opportunity to discuss the timbre space in representations? Representing synthesizer sounds or something?
% % - Research by Benjamin Hayes on the timbre space of synths -- looks super interesting.
% % - https://benhayes.net/assets/pdf/16.Hayes.pdf
% % - https://benhayes.net/assets/pdf/icmpc_escom_2021.pdf
% % - https://benhayes.net/publications/ (Has some interesting perceptual studies on synthesizer timbre space, also something new on neural synthesis)


% Since the early 90s, researchers have leveraged advances in ML to develop a deeper understanding of the synthesizer parameter space and to build more intuitive methods for interaction \cite{horner1993machine}. Recently, deep learning has been used for programming synthesizers.  Esling {\em et al.}\ \cite{esling2020flow} trained an auto-encoder network to program the \href{https://u-he.com/products/diva/}{U-He Diva} using 11K synthesized sounds with known preset values. Yee-King {\em et al.} \cite{yee2018automatic} used a recurrent network to automatically find parameters for \href{https://asb2m10.github.io/dexed/}{Dexed}, an open-source software emulation of the DX7.

% - In this section an overview of the field of automatic synthesizer programming is presented. Specific emphasis is placed on the HCI aspect of the problem and an overview of various user interaction methodologies is provided here.
% - Inverse synthesis, a subset of automatic synthesizer programming and a focus of this thesis, is reviewed in more detail in the following section.
% - This chapter concludes with a categorization of work in automatic synthesizer programming research that has been conducted over the last 30 years.

%--------------------------------------------------------------------

\section{Background}
Automatic synthesizer programming is an inherently interdisciplinary field. Previous research has focused on both the algorithmic and technical aspects of the problem, as well as the human interaction aspects of the problem. Depending on the focus a solution may draw from fields including human-computer interaction, digital signal processing (DSP), machine learning and artificial intelligence, to name a few. Insights can also be gleaned from related fields including music information retrieval and intelligent music production. To provide context and situate automatic synthesizer programming in a wider body of research, a brief introduction to these fields and how they relate to the synthesizer programming problem is provided in this section.

\subsection{Human Computer Interaction}
\subsubsection{Creative Support Tools}
Automatic synthesizer programming can be viewed from the perspective of trying to solve a human computer interaction (HCI) problem. How can a synthesizer user more effectively communicate their creative ideas to a synthesizer? Currently users are expected to learn the domain language from the perspective of whatever synthesis algorithm they are using. Can a user instead communicate their ideas to a synthesizer in a way that fits their creative needs? Creativity support is an emerging field of study that is interested in answering questions similar to this. It is focused on the development of tools that enable and enhance the creative output of an individual or group, both novices and experts. Creativity support tools (CSTs) \cite{shneiderman2007creativity} span a wide array of application domains including visual art, textiles, cooking, and music. A central question that CSTs ask is: 
\begin{quote}
    "How can designers of programming interfaces, interactive tools, and rich social environments enable more people to be more creative more often?"
\end{quote}

Shneiderman \cite{shneiderman2007creativity} outlines a set of design principles for developing creativity support tools which include: support exploratory search, enable collaboration, provide rich history keeping, design with low thresholds, high ceilings, and wide walls. In subsequent related work, Davis \textit{et al.} focus on the role that CSTs play in supporting novices engaging in creative tasks and the relationship that the environment plays in creativity \cite{davis2013toward}. In their work, the authors identify two types of novice users: domain novices and tool novices. Domain novices are new to both the creative domain as well as using the creativity support tool. Tool novices have experience with the creative domain, but are novices at using a particular tool. To help evaluate and promote the development of creativity support tools for novices, they also propose a theory of creativity support based on cognitive theory.

These concepts provide an important platform for beginning to develop tools to support users of synthesizers. Both types of novices described by Davis \textit{et al.} are common and serve to benefit from the development of improved methods for interacting with them; domain novices are both new to sound design / music production as well as to using a specific synthesizer, whereas a tool novices would likely have experience with sound design / music production, but would be a novice with using a specific synthesizer.

\subsubsection{Music Interaction}
Music and Human-Computer Interaction \cite{holland2013music}, or Music Interaction. Research related to the use of interactive systems that involve computes for any kind of musical activity. Music interaction draws heavily from other areas of HCI research, but also responds to the needs and desires of the music community. There are unique considerations in the context of musical applications that makes music interaction different from other fields of HCI. A musical instrument is not a utilitarian tool whose development should be ever-improved to make it more efficient. Musical instruments are played, and sometimes that is the only goal. Tomaka [cite me!] identifies that imperfections and limitations of a musical instrument give an instrument character. McDermott \cite{mcdermott2013should} identifies the importance of engagement in musical interaction and the relation that bears to the concept of \textit{flow} \cite{csikszentmihalyi1990flow}. The learning curve plays is crucial to the level of engagement that a player experiences when playing a musical instrument, both in the short-term and long-term. Holland \cite{holland2013music} concludes, "In order to remain engaging, consuming and flow-like, activities that involve musical instruments must offer continued challenges at appropriate levels of difficulty: not too difficult, and not too easy."

\subsubsection{Music Production Tools}
Related to musical instruments are tools that are used in the process of recording and producing musical recordings. Music producers often use a collection of computer-based music production tools including DAWs, audio plugins, and synthesizers to create their musical compositions. In fact, the term \textit{in-box} has been coined to refer to the process of creating a piece of music from start to finish completely within a computer. With the rise of more music production tools and libraries of sounds for creating music with (often referred to as audio sample libraries), producers have started to express desire for improved methods of interaction. Kristina Andersen conducted a set of interviews with expert music producers at the Red Bull Music Academies in 2014 and 2015, and asked them about their experience with their music production tools \cite{andersen2016conversations}. One of the main challenges expressed was related to navigating large collections of audio libraries, which could contain half a million or more individual audio files. A main finding that reflects some of the principles of music interaction is that some participants relied on idiosyncratic methods as well as randomness; serendipity played a role in their creative process and they expressed a desire for tools that would support that. Another interesting finding is that users often had a mental image of their creative vision, creating tools that could interpret those, whether through semantic descriptions, colours, images, or haptics would be a valuable interaction paradigm. 

Pardo \textit{et al.} \cite{pardo2019learning} breakdown the current interaction paradigm for music production tools into three different conceptual spaces: the perceptual space, the parameter space, and the semantic space. The perceptual space is related to how a tool actually sounds when it is being applied, i.e. we can tell the different when the filter on synthesizer is changed. The parameter space is the actual low-level techniques parameters on the control interface, i.e. the filter was parameter changed from 500Hz to 2000Hz. The semantic space is related to how one would describe the changes that we hear, e.g the synthesizer sounds brighter as a result of the filter change. Users are asked to bridge the gap between the low-level parameter space and the higher-level perceptual and semantic spaces themselves, which often involves a complex and non-linear mapping. Pardo \textit{et al.} identify the novice user as being particularly challenged by gap between the low-level and high-level spaces, and present a framework for developing tools that help to bridge that gap. They propose four different methods for doing so: 1) Evaluation (allow the user to listen and evaluate a set of proposed variations), 2) using descriptive words, 3) using vocal imitations, and 4), exploration.

% This creates some design motivation / questions specific for ASP.
\subsubsection{HCI and Automatic Synthesizer Programming}
The inverse synthesis approach seeks to completely abstract the process of programming a sound. Creating a new sound with a synthesizer is seen as a hindrance to the process of creating music. But people enjoy using synthesizers and being involved in the process of designing new sounds. They already support long-term engagement to the dedicated users. The question arises, how can we support creativity and engagement for users of all experience levels? Can interfaces be designed that offer continued challenges at the appropriate level of difficulty? Can these interfaces adapt to the correct level of difficultly for a particular user?

With the mass digitization of musical recordings and the shift to producing music with computers, questions around how we interact with music via computers have grown. Several related fields of study, adjacent to automatic synthesizer programming, have emerged over the years in attempt to answer these questions. Contributions from these fields provide insight into the larger trends in music interaction as well as provide inspiration for future directions in synthesizer programming. Two particularly related fields worth mentioning is music information retrieval (MIR) and intelligent music production.

\subsection{Music Information Retrieval}
MIR is a growing research area that was born out of the need to navigate increasingly large collections of digital music. The field centers around the International Society of Music Information Retrieval (ISMIR), created through the merging of the fields of symbolic music and audio signal processing in 1999, hosted their first annual symposium in 2000 \cite{downie2009ten}. One of the main research goals of MIR is to develop systems to support users in organizing and querying large collections of digital audio. A number of unique solutions to querying audio databases have been proposed including query-by-example \cite{zloof1977query}, query-by-vocal-imitation \cite{blancas2014sound}, and query-by-beat-boxing \cite{kapur2004query}. Automatic synthesizer programming can be viewed as a query retrieval task where the product being retrieved is a parameter setting for a particular synthesizer. Query-by-example is directly related to the inverse synthesis problem and others have proposed query-by-vocal-imitation for automatic synthesizer programming as well \cite{cartwright2014synthassist}. 

Visualization of audio databases to support navigation and browsing is another established area of research in MIR. One of the first researchers to explore visualizations of audio based on timbre was Grey \cite{grey1977multidimensional} in 1977, followed by subsequent work by Wessel in 1979 \cite{wessel1979timbre}. The concept of visualization in MIR is comprehensively reviewed by Cooper et al. \cite{cooper2006visualization}, and is updated by Schedl et al. to represent contemporary work \cite{schedl2014music}. AudioQuilt is a notable example of an experimental application for visualizing audio samples \cite{fried2014audioquilt}. AudioQuilt uses metric learning and kernalized sorting algorithms to visualize audio samples in two dimensions. Tzanetakis and Cook \cite{tzanetakis20003d} review experimental applicationts with novel user interfaces for exploring sound collections in three dimensions. A unique approach to sample retrieval is proposed by Knees and Andersen \cite{knees2016searching}, in which users query audio using visual sketches of their mental images of that sound.  Visualization of synthesizer sounds is explored in chapter 

\subsection{Creative MIR and Intelligent Music Production}
Creative MIR is a subset of MIR that is focused on the applying techniques from the field towards creative applications including music production \cite{humphrey2013brief}. Music production tools, including tools to that support querying and browsing of a audio samples and synthesizing new sounds are an aspect of this. Expert music producers have expressed desires for improved methods for working with large collections of audio samples \cite{andersen2016conversations} as well as for the development of systems that would help generate novel ideas. 

Music producers have expressed desires for improved methods for working with  Related work that I completed during my time at University of Victoria focused on techniques for visualizing drum machine samples in a music production context \cite{shier2021manifold}. Intelligent music production \cite{moffat2019approaches} is another related field that draws from MIR and machine learning with the aim to develop tools that support creativity and automate workflows within the context of music production. Tools that automate mixing, the practice of combining multiple individual audio tracks into a single tack, is a complex task that has received a large amount of focus \cite{de2017ten}.

\subsection{Neural Synthesis}
One area of new development in audio synthesis is in methods that are leveraging advancements from the field of deep learning [deep learning cite], an area of audio synthesis that is explored in this thesis.  
In contrast to traditional synthesis, neural synthesizers generate audio using large-scale machine learning architectures with millions of parameters \cite{engel2017neural}. Differentiable digital signal processing \cite{engel2020ddsp} bridged the gap between traditional DSP synthesizers with the expressiveness of neural networks, exploring a harmonic model-based approach, using a more compact architecture with 100K parameters.
One benefit of synthesized audio is that the underlying factors of variation ({\em i.e.}~the parameters) are known.

% GANs for synthesis
In this work, we use Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} to generate new instrumental audio from a dataset of existing material. GANs have the potential to be used to generate new sounds on the fly. This would dramatically alleviate both the problem of having to pore through giant sound libraries, and the problem with having to only use one sample repeatedly. In addition, the explosion of new sounds which could potentially be produced by GANs would vastly reduce recording costs by designers of sound libraries.

This research avenue is to a certain degree untapped: GANs have been successfully applied to the generation and manipulation of images, however, relatively little work has been focused on the audio domain. Research related to the specific work proposed here was presented by \cite{donahue2018adversarial}  and \cite{engel2018gansynth}.
\cite{ccakir2018musical} - not totally sure what this is about, I think it is generative though.

%--------------------------------------------------------------------

\section{Approaches}
In an automatic synthesizer programming system the result is typically an additional layer of abstraction on top of the control interface. The goal of this abstraction is to help bridge the conceptual gap between the parameter space and the perceptual and semantic space. Similar to how synthesizer control interfaces create a mapping from the parameter space to the synthesis engine, interfaces of automatic synthesizer programming systems present an abstracted higher-level model of the parameter space to a user and create a mapping to the parameter space. Figure [Insert figure that builds off of the one some the synthesizer chapter] shows the original components of an audio synthesizer (the synthesis engine and control interface), along with the additional automatic synthesizer programming interface.

A number of different interface approaches have been explored in previous ASP research. Pardo \textit{et al.} \cite{pardo2019learning} presented a framework for classifying different interaction paradigms within audio production tools. The development of this framework was based on how musicians and audio engineers communicate auditory concepts and included four different types of interaction styles: 1) evaluation interfaces, 2) using descriptive words, 3) vocal imitations, and 4) exploration interfaces. Pardo \textit{et al.} evaluated four different examples of audio production software with \textit{natura} interfaces that attempted to bridge the gap between the parameter space and perceptual/semantic space. One thing to note is that a system may fall under one or more of these categorizations.

These four categories are useful for understanding the different approaches to ASP interactions that have been proposed in previous work. In addition to those, we also include a fifth category: \textit{example-based} interfaces. 

\subsubsection{Example-based Interfaces}
Example-based interfaces allow a user to present a system with an auditory example of a sound that they wish to have programmed into their synthesizer. This style of interface is a superset of vocal imitation interfaces for automatic synthesizer programming as vocal imitations represent a specific style of providing an example to a system. Inverse synthesis is an example of an example-based interaction paradigm and this approaches make up a large portion of the previous work in ASP.


\subsection{Evaluation Interfaces}
Evaluation type interfaces allow users to compare different potential synthesizer sounds and iteratively work towards finding their ideal solution. 


In 1986 Ashley proposed one of the first examples of an system for programming a synthesizer using semantic descriptions of the desired timbre \cite{ashley1986knowledge}.


- This is really a user interface problem. Programming synthesizers is hard!
- What are some of the different approaches that have been taken by people?
- Interactive methods (IGAs)
- Semantic search
- That interesting paper on sketching sounds visually
- Pardo look at how we can build more natural interfaces for audio production tools \cite{pardo2019learning}

\subsection{Interactive Searches}
Researchers have also used Interactive Genetic Algorithms (IGAs) that allow users to interactively hear and rate potential synthesizer patches \cite{johnson1999exploring, dahlstedt2001creating, yee2016use}. In contrast to the sound matching case, the evaluation function in an IGA relies on user feedback during each iteration as opposed to measuring error between a candidate and a target. 

Reinforcement Learning and interaction \cite{scurto2021designing}

 \subsection{Vocal Imitations}
 It has been shown that vocal imitations are promising way to communicate sound concepts \cite{lemaitre2014effectiveness} and the VocalSketch dataset has been released to further research in this area \cite{cartwright2015vocalsketch}. Systems using vocal imitations include \cite{mcartwright2014}\cite{zhang2018visualization}. Other systems rely solely on human feedback in order to optimize towards a goal sound starting from a random selection of synthesizer patches. 
 
 \subsection{Semantic}
 Automatic programming using semantic sound descriptions has also been explored, and is a further methodology that has used GAs \cite{krekovic2016algorithm}.
 - Check the seago cite?
 \cite{roche2021make} - VAE
 
 Other:
 
 Sketching sounds \cite{lobbers2021sketching} \cite{knees2016searching}
 
 Timbre space representation of a substractive synthesizer: \cite{vahidi2020timbre} (Read this, it's short, how does this fit into the landscape?)

%--------------------------------------------------------------------

\section{Inverse Synthesis}

Early research in automatic synthesizer programming conducted in the late 70s through to the early 90s focused on using analytic methods to understand and reverse engineer parameter settings for FM synthesis \cite{justice1979analytic, beauchamp1982synthesis, payne1987microcomputer} and non-linear synthesis \cite{delprat1990parameter} algorithms. 

- inverse synthesis is the problem of estimating parameters for a synthesizer to match a target sound.
- This is the main issue that has been explored in the literature and is a big component of the research that is conducted here
 
This section provides a brief summary of the main algorithmic methodologies that have been used in previous ASP research, namely, optimization and deep learning techniques. Other methods that have been used in ASP research that are beyond the scope of this paper include  include fuzzy logic \cite{mitchell2005frequency, hamadicharef2012intelligent}, linear coding \cite{mintz2007toward}, and query approaches \cite{mcartwright2014}.



% This could potentially be its own section
\subsection{Audio Representations}
- how do we represent audio digitally?
- That tutorial by that ableton author was helpful -- maybe can link to that.
- Don't go into too much depth here.
- What is a perceptually relevant method for representing audio?
- Spectral features
- STFT

% Not totally sure how to incorporate these sections?
\subsubsection{Learned Audio Representations}
- Representation learning \cite{bengio2013representation}
- Introduce machine learning
- self-supervised learning
- Learned audio representations
- Give a brief plug for HEAR 2021 and the need for perceptual audio representations

\subsection{Search vs. Modelling}
% Add something abouth the search vs. modelling approach
These two methods, the hill-climber and the LSTM++, represent two different methods for inverse synthesis; the hill-climber is a search algorithm and the LSTM++ is a modelling algorithm. Search algorithms (which include genetic algorithms) have been used extensively in the body of automatic synthesizer programming research and modelling with deep learning has been becoming more popular. Search algorithms are presented with a target sound and then begin an iterative search for the parameter settings, attempting to move closer to the target at each iteration.

\subsection{Optimization}
The optimization approach was first introduced in 1993 with Horner et al.'s work on sound matching for FM synthesis using genetic algorithms \cite{horner1993machine}. A genetic algorithm (GA) is a method for solving an optimization problem using techniques based on the principles of Darwinian evolution, and is part of a broader class of evolutionary algorithms \cite{whitley1994genetic}. In a GA, a potential solution (an individual) is represented as an array of bits. An initial set of individuals is randomly generated, and then iteratively evolved using biologically inspired processes including selection, breeding, and mutation. Individuals are ranked using an evaluation function that measures the $fitness$ of a given solution. The objective of a GA is to minimize that value (or maximize it, depending on the problem definition). The best candidates are selected for further evolution until either an optimal solution is found or a set number of evolutions has been completed.

In the case of sound matching, the \textit{fitness} of a potential solution is determined by measuring the error between a target sound and a candidate. Typically, an audio transform or audio feature extraction is performed prior to calculating \textit{fitness}. The first works on synthesizer sound matching with GAs used the Short Time Fourier Transform (STFT) in the evaluation function \cite{horner1993machine, horner1995wavetable}. Mel-frequency Cepstral Coefficients (MFCCs), an audio representation using a non-linear frequency scaling that is more relevant to human hearing, have also been used \cite{yee2008synthbot, roth2011comparison, macret2014automatic, smith2017play}. Tatar et al. introduced the use of a multi-objective GA for synthesizer sound matching that used three different methods for calculating $fitness$ values: the STFT, Fast Fourier Transform (FFT), and signal envelope \cite{tatar2016automatic}. Alternatives to GAs that have been used for sound matching include Particle Swarm Optimization (PSO) \cite{heise2009automatic} and Hill-Climbing \cite{roth2011comparison, luke2019stochastic}.

\subsection{Deep Learning}
Deep learning is subset of machine learning that utilizes artificial neural networks to learn patterns in data and make predictions based on those patterns \cite{lecun2015deep}. Deep learning architectures contain multiple layers comprised of simple non-linear modules. Through iterative training, the layers are able to extract features from raw input data and learn intricate patterns in high-dimensional data. These multi-layer architectures have enabled deep learning models to excel at complex tasks including image recognition, speech recognition, and music related tasks such as audio source separation \cite{spleeter2019}.

In the context of an ASP sound matching experiment, a deep learning model accepts an audio signal as input and predicts synthesizer parameter settings to replicate that audio signal. Audio signals are often preprocessed using audio feature extraction or an audio transform. Models are trained using a large set of example sounds generated from a synthesizer and use the parameter settings that generated a particular sound as the ground truth. During training, the error between predicted parameter settings and the actual parameter settings (the ground truth) are used to evaluate how well the model is learning and to iteratively update variables within the model to improve performance. 

Several researchers have explored the use of deep learning for ASP. Yee-King et al. reviewed several deep learning architectures for FM synthesizer sound matcing \cite{yee2018automatic}. In their work, they compared multi-layer perceptron (MLP), Long Short Term Memory (LSTM), and LSTM++ networks. Barkan et al. explored sound matching using convolutional neural networks (CNNs) \cite{barkan2019deep, barkan2019inversynth}. They framed the problem as an image classification task and used the STFT to create spectrogram images of target sounds to use as input to the CNNs. Esling et al. recently presented a novel application called $FlowSynth$ that uses a generative model based on Variational Auto-Encoders and Normalizing Flows \cite{esling2020flow}. In addition to performing well on sound matching tasks, they also showed that their approach supported novel interactions including macro-control of synthesizer parameters.

\cite{le2021improving} -- variational autoencoder. Generative approach to the synth browsing problem. Want to read this.

\cite{mitcheltree2021serumrnn} -- ensemble of models that work together to iteratively apply audio effects and select parameters for those effects to reduce the error between and input and a target. Significantly reduces the error between the input and target audio. Benefits from applying effects iteratively in a specific order. Learns which effects are most important. Provides interpretable and valuable intermediate steps. Can discover more efficient effect order sequences than a variety of baselines.

\cite{masudo2021quality} - GA based sound matching.

%--------------------------------------------------------------------

\section{Challenges and Opportunities}
- Okay can I motivate future work here?
- The parameter space is ginormous! How do we represent synthesized sounds in a perceptually relevant way? The relationship betweent parameters and the resulting output sounds is very complex. Deep learning 

% Challenges. Maybe move this to a section that is specifically related to automatic synthesizer programming challenges.
% This is a challenge that is potentially particularly related to synthesizer programming. The space of possible sounds is immense! It is infeasible to compute all the possible sounds and index them beforehand. So the question becomes, how to you navigate this space in an intelligent way?? How do we learn to teach our systems to generate sounds or learn to programm sounds in a way that is related to or relevant to how a human would program a sound?? This is addressed in the torchsynth chapter a bit.
One of distinguishing factors between querying tasks and automatic synthesizer programming is that often querying tasks, the database of audio already exists and can be queried for an exact match or for the match that is most similar. Some approaches to automatic synthesizer programming have pre-computed large sets of sytnhesizer sounds 