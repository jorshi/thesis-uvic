\chapter{Background}
\label{chapter:background}
- Research in this field is inherently interdisciplinary background covers many different areas including HCI, DSP, Machine learning
- What is the main goal? Reiterate here perhaps: "We want to develop a method for improving the user interface for synthesizer users" -- 
- In the field of HCI the study of creativity support tools is related to this:

The field of creativity support is focused on the development of tools to enable and enhance the creative output of an individual or group, both in novice and expert users. Creativity support tools (CSTs) \cite{shneiderman2007creativity} span a wide array of application domains including visual art, textiles, cooking, and music. In their paper on the topic Schneiderman asks the question: "How can designers of programming interfaces, interactive tools, and rich social environments enable more people to be more creative more often?". They outline a set of design principles for developing creativity support tools which include: support exploratory search, enable collaboration, provide rich history keeping, design with low thresholds, high ceilings, and wide walls. In subsequent related work, Davis \textit{et al.} focus on the role that CSTs play in supporting novices engaging in creative tasks and the relationship that the environment plays in creativity \cite{davis2013toward}.

- More directly related to music the fields of MIR (creative MIR) as well as intelligent music production. - automatic mixing ref, other topics (hit up some references and expand on this)

% Segway much??
A common tool in audio production, and the focus of this work, is the audio synthesizer.

\section{Synthesizers}
- What are synthesizers?
- Why are they useful? What do we use them for?
- Sound design
- Give a brief history on synthesizers (like a couple sentences) east coast vs. west coast synthesis. FM synthesizers.
- Distinguish between hardware synthesizers and software synthesizers
- Introduce audio plugins and software synthesizers
- Neural Synthesis
- DDSP
- Touch on programming synthesizers and the challenges involved.

\section{Digital Audio}
- how do we represent audio digitally?
- That tutorial by that ableton author was helpful -- maybe can link to that.
- Don't go into too much depth here.

\subsection{Audio Representations}
- What is a perceptually relevant method for representing audio?
- Spectral features
- STFT

\subsection{Audio Machine Learning}
- Introduce machine learning
- self-supervised learning
- Learned audio representations
- Give a brief plug for HEAR 2021 and the need for perceptual audio representations

\section{Synthesizer Programming}
- What is automatic synthesizer programming? Surely I have some good material
- Distinguish between the user interface and the algorithm (frontend vs. backend)
- Research by Benjamin Hayes on the timbre space of synths -- looks super interesting.

\section{Inverse Synthesis}
- inverse synthesis is the problem of estimating parameters for a synthesizer to match a target sound.
\subsection{Search vs. Modelling}

% Add something abouth the search vs. modelling approach
These two methods, the hill-climber and the LSTM++, represent two different methods for inverse synthesis; the hill-climber is a search algorithm and the LSTM++ is a modelling algorithm. Search algorithms (which include genetic algorithms) have been used extensively in the body of automatic synthesizer programming research and modelling with deep learning has been becoming more popular. Search algorithms are presented with a target sound and then begin an iterative search for the parameter settings, attempting to move closer to the target at each iteration.

% What's this text from?
Automatic synthesizer programming can be viewed as a retrieval task as well as an optimization problem. Viewed as a retrieval task, the problem is similar to the MIR query tasks such as Query-by-example \cite{zloof1977query}, Query-by-vocal-imitation \cite{blancas2014sound}, and query-by-beat-boxing \cite{kapur2004query}. Query problems generally build up a model of the synthesis parameter space and then return a parameter setting based on a classification that attempts to match the input with the best parameter setting. The optimization approach is similar to the retrieval task in that the goal of the system is to return a synthesizer parameter setting that represents the input query as closely as possible. In contrast to the retrieval approach, instead of modelling the synthesizer parameter space, optimization algorithms will incrementally and automatically "turn the knobs" on a given synthesizer until an appropriate setting has been found. Genetic algorithms have been found to be particularly useful in this field and have been used in many studies. Other approaches such as Particle Swarm Optimization and Evaluation Strategies have also been implemented. 

 Most studies have focussed on the sound matching programming task, in which an example sound is provided to the system with the goal of reproducing that sound on the target synthesizer. Others have focussed on using a semantic description of the goal sound as input to the programming interface. It has been shown that vocal imitations are promising way to communicate sound concepts \cite{lemaitre2014effectiveness} and the VocalSketch dataset has been released to further research in this area \cite{cartwright2015vocalsketch}. Systems using vocal imitations include \cite{mcartwright2014}\cite{zhang2018visualization}. Other systems rely solely on human feedback in order to optimize towards a goal sound starting from a random selection of synthesizer patches.
 
 % SpiegeLib Text
This section provides a brief summary of the main algorithmic methodologies that have been used in previous ASP research, namely, optimization and deep learning techniques. Other methods that have been used in ASP research that are beyond the scope of this paper include  include fuzzy logic \cite{mitchell2005frequency, hamadicharef2012intelligent}, linear coding \cite{mintz2007toward}, and query approaches \cite{mcartwright2014}. An  informal survey of open-access software that supports reproducibility is also included at the end of the section.

\subsection{Optimization}
The optimization approach was first introduced in 1993 with Horner et al.'s work on sound matching for FM synthesis using genetic algorithms \cite{horner1993machine}. A genetic algorithm (GA) is a method for solving an optimization problem using techniques based on the principles of Darwinian evolution, and is part of a broader class of evolutionary algorithms \cite{whitley1994genetic}. In a GA, a potential solution (an individual) is represented as an array of bits. An initial set of individuals is randomly generated, and then iteratively evolved using biologically inspired processes including selection, breeding, and mutation. Individuals are ranked using an evaluation function that measures the $fitness$ of a given solution. The objective of a GA is to minimize that value (or maximize it, depending on the problem definition). The best candidates are selected for further evolution until either an optimal solution is found or a set number of evolutions has been completed.

In the case of sound matching, the \textit{fitness} of a potential solution is determined by measuring the error between a target sound and a candidate. Typically, an audio transform or audio feature extraction is performed prior to calculating \textit{fitness}. The first works on synthesizer sound matching with GAs used the Short Time Fourier Transform (STFT) in the evaluation function \cite{horner1993machine, horner1995wavetable}. Mel-frequency Cepstral Coefficients (MFCCs), an audio representation using a non-linear frequency scaling that is more relevant to human hearing, have also been used \cite{yee2008synthbot, roth2011comparison, macret2014automatic, smith2017play}. Tatar et al. introduced the use of a multi-objective GA for synthesizer sound matching that used three different methods for calculating $fitness$ values: the STFT, Fast Fourier Transform (FFT), and signal envelope \cite{tatar2016automatic}. Alternatives to GAs that have been used for sound matching include Particle Swarm Optimization (PSO) \cite{heise2009automatic} and Hill-Climbing \cite{roth2011comparison, luke2019stochastic}.

Researchers have also used Interactive Genetic Algorithms (IGAs) that allow users to interactively hear and rate potential synthesizer patches \cite{johnson1999exploring, dahlstedt2001creating, yee2016use}. In contrast to the sound matching case, the evaluation function in an IGA relies on user feedback during each iteration as opposed to measuring error between a candidate and a target. 

Automatic programming using semantic sound descriptions has also been explored, and is a further methodology that has used GAs \cite{krekovic2016algorithm}.

\subsection{Deep Learning}
Deep learning is subset of machine learning that utilizes artificial neural networks to learn patterns in data and make predictions based on those patterns \cite{lecun2015deep}. Deep learning architectures contain multiple layers comprised of simple non-linear modules. Through iterative training, the layers are able to extract features from raw input data and learn intricate patterns in high-dimensional data. These multi-layer architectures have enabled deep learning models to excel at complex tasks including image recognition, speech recognition, and music related tasks such as audio source separation \cite{spleeter2019}.

In the context of an ASP sound matching experiment, a deep learning model accepts an audio signal as input and predicts synthesizer parameter settings to replicate that audio signal. Audio signals are often preprocessed using audio feature extraction or an audio transform. Models are trained using a large set of example sounds generated from a synthesizer and use the parameter settings that generated a particular sound as the ground truth. During training, the error between predicted parameter settings and the actual parameter settings (the ground truth) are used to evaluate how well the model is learning and to iteratively update variables within the model to improve performance. 

Several researchers have explored the use of deep learning for ASP. Yee-King et al. reviewed several deep learning architectures for FM synthesizer sound matcing [11]. In their work, they compared multi-layer perceptron (MLP), Long Short Term Memory (LSTM), and LSTM++ networks. Barkan et al. explored sound matching using convolutional neural networks (CNNs) [12]. They framed the problem as an image classification task and used the STFT to create spectrogram images of target sounds to use as input to the CNNs. Esling et al. recently presented a novel application called $FlowSynth$ that uses a generative model based on Variational Auto-Encoders and Normalizing Flows [13]. In addition to performing well on sound matching tasks, they also showed that their approach supported novel interactions including macro-control of synthesizer parameters.

\subsection{Evaluation Methods}
% Maybe? A brief review of qualitative vs. quantiative methods?
 
\section{User Interface Design}
- What types of user interfaces have been developed that are interesting?
- Include some screenshots here
- Vocal imitations
- Interactive IGAs
- VocalSketch
- Semantic queries
 
\section{Forty Years of Automatic Synthesizer Programming}
- organized time of all the related work. Organized based on: synthesis type and method.
- Also include a separate table for evaluation methods (maybe not totally necessary).

\begin{itemize}
	\item \cite{zloof1977query} Early example of querying sounds (need to read)
	\item \cite{justice1979analytic} Coarse parameter matching by analyzing input audio - goal was to get in the ballpark and allow for parameter tweaking afterwards. FM (Single carrier with nested modulators). Hilbert Transform. Objective evaluation.
	\item \cite{wessel1979timbre} Not specifically for synthesis methods but classic paper. This should come much earlier in this section.
	\item \cite{beauchamp1982synthesis} Matching of alto sax/cornet sounds using an analytic method for FM synthesis. Looked at spectral centroid and RMS. Objective evaluation.
	\item \cite{ashley1986knowledge} A knowledge-based approach to assistance in timbral design (need to read still)
	\item \cite{payne1987microcomputer} Hilbert Transform (time domain). Also introduced FFT version with autocorrelation on spectrum. Periodic sampled sounds, FM DX7, objective evaluation.
	\item \cite{delprat1990parameter} Parameter estimation for non-linear resynthesis methods with the help of a time-frequency analysis of natural sounds.
	\item \cite{horner1993machine} Genetic algorithms for inverse synthesis of instrumental sounds on FM synthesis engine.
	\item \cite{horner1993methods} Wavetable
	\item \cite{vuori1993parameter} Parameter estimation of non-linear physical models by simulated evolution-application to the flute model
	\item \cite{takala1993using} Using Physically-Based Models and Genetic Algorithms for Functional Composition of Sound Signals
	\item \cite{fujinaga1994genetic} Genetic algorithms as a method for granular synthesis regulation
	\item \cite{ethington1994seawave} Semantic search -- this is an good one, as far as I can tell so far this is the first semantic search for synthesizer sounds.
	\item \cite{miranda1995artificial} An artificial intelligence approach to sound design
	\item \cite{horner1995wavetable} Updated version of wavetable matching. Used multiple pitches of instrumental tones and a genetic algorithm.
	\item \cite{horner1995envelope} Envelope matching with genetic algorithms
	\item \cite{horner1996computation} Computation and memory tradeoffs with multiple wavetable interpolation
	\item \cite{horner1996piecewise} Piecewise-linear approximation of additive synthesis envelopes: a comparison of various methods
	\item \cite{cheung1996group} Group synthesis (wavetable) with genetic algorithms
	\item \cite{horner1996double} Double-modulator FM matching of instrument tones
\end{itemize}


\subsection{Synthesis Type}
An overview of synthesis type that was the focus of automatic synthesizer programming studies.

\subsubsection{FM}
\cite{justice1979analytic}\cite{beauchamp1982synthesis}\cite{payne1987microcomputer}\cite{horner1993machine}\cite{horner1996double}\cite{tan1996automated}\cite{delprat1997global}\cite{lim1999performance}\cite{tan2003automated}
\cite{mitchell2005frequency}\cite{mitchell2007evolutionary}\cite{clement2011automatic}\cite{roth2011comparison}\cite{macret2012automatic}\cite{hamadicharef2012intelligent}\cite{barkan2019deep}

\subsubsection{Non-linear}
\cite{beauchamp1982synthesis}\cite{delprat1990parameter}

\subsubsection{Wavetable / Group Synthesis}
\cite{horner1993methods}\cite{horner1995wavetable}\cite{horner1995envelope}\cite{horner1996computation}\cite{horner1996piecewise}\cite{cheung1996group}\cite{oates1997analytical}\cite{horner1998modeling}\cite{lee1999modeling}\cite{so2002wavetable}

\subsubsection{Physical Modelling}
\cite{vuori1993parameter}\cite{erkut2000extraction}\cite{liang2000recurrent}\cite{nackaerts2001parameter}\cite{riionheimo2003parameter}

\subsubsection{Granular}
\cite{fujinaga1994genetic}\cite{johnson1999exploring}

\subsubsection{Additive}
\cite{ethington1994seawave}\cite{horner1995envelope}\cite{horner1996piecewise}\cite{johnson2006timbre}\cite{mintz2007toward}

\subsubsection{Subtractive}
\cite{roth2011comparison}

\subsubsection{Generic VST}
\cite{yee2008synthbot}\cite{heise2009automatic}

\subsubsection{Other}
Noise-band \cite{chinen2007genesynth}
Concatenative \cite{stowell2010making}
Teenage Engineering OP-1 (Multiple Synthesis Engines) \cite{macret2013automatic}

\subsection{Estimation Method}
An overview of the method used to select a synthesizer parameter setting based on input.

\subsubsection{Analytic / Signal Processing}
\cite{justice1979analytic}\cite{beauchamp1982synthesis}\cite{payne1987microcomputer}\cite{ethington1994seawave}

\subsubsection{Genetic}
\cite{horner1993machine}\cite{fujinaga1994genetic}\cite{horner1995envelope}\cite{horner1995wavetable}\cite{riionheimo2003parameter}\cite{mandelis2003musical}\cite{mitchell2005frequency}\cite{mitchell2007evolutionary}
\cite{chinen2007genesynth}\cite{yee2008synthbot}\cite{roth2011comparison}\cite{macret2012automatic}\cite{hamadicharef2012intelligent}\cite{macret2013automatic}

\subsubsection{Interactive Genetic}
\cite{johnson1999exploring}

\subsubsection{Neural Network}
\cite{johnson2006timbre}\cite{roth2011comparison}\cite{zhang2018visualization}\cite{barkan2019deep}

\subsubsection{Data-driven}
\cite{roth2011comparison}\cite{mcartwright2014}

\subsubsection{Other}
Linear coding \cite{mintz2007toward}
Particle Swarm Optimization \cite{heise2009automatic}\cite{munoz2011opposition}
Regression Tree \cite{stowell2010making}
Semantic Clustering \cite{clement2011automatic}
Hill-Climber \cite{roth2011comparison}

\subsection{Unsorted}
These references have not been reviewed or sorted yet!

\cite{takala1993using}
\cite{hourdin1997sound}
\cite{horner1998nested}
\cite{wehn1998using}
\cite{garcia2001automatic}
\cite{dahlstedt2001creating}
\cite{garcia2001growing}
\cite{jehan2001perceptual}
\cite{su2002class}
\cite{le2002neural}
\cite{arfib2002strategies}
\cite{miranda2004crossroads}
\cite{schatter2005synaesthetic}
\cite{gounaropoulos2006synthesising}
\cite{lai2006automated}
\cite{mcdermott2007evolutionary}
\cite{yee2007automated}
\cite{yee2007evolving}
\cite{howard2007timbral}
\cite{mcdermott2008evolutionary}
\cite{yee2011automatic}
\cite{mitchell2012automated}
\cite{povscic2013controlling}
\cite{seago2013new}
\cite{krekovic2014intelligent}
\cite{macret2014automatic}
\cite{itoyama2014parameter}
\cite{huang2014active}
\cite{fasciani2016tsam}
\cite{krekovic2016algorithm}
\cite{tatar2016automatic}
\cite{yee2016use}
\cite{smith2017play}
\cite{yee2018automatic}
\cite{luke2019stochastic}
