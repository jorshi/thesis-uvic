\chapter{Conclusions} % and Future Work}
This thesis has explored the topic of automatic synthesizer programming and has presented work to support continued research in this field. The main research question that was asked at the beginning of this thesis was: "How can designers of synthesizer programming interfaces enable more people to be more creative more often?" Automatic synthesizer programming seeks to answer this question through research focused on providing more intuitive methods for users to communicate their ideas to their synthesizers.

Chapter \ref{chapter:background} provided a background of synthesizers and discussed the specific challenges associated with synthesizer programming. The core of these challenges arises from the disconnect between the parameters used to control a synthesizer and the associated auditory result. The conceptual gap created by this disconnect is large and, accordingly, synthesizer user face a steep learning. 

Automatic synthesizer programming research has explored a number of methods for aiding users to more easily translate their desired audio results into synthesizer parameters. Chapter \ref{chapter:asp-background} provided a survey of this research. Six different automatic synthesizer user interaction styles were identified and reviewed: 1) example-based interfaces, 2) evaluation interfaces, 3) using descriptive words, 4) vocal imitations, 5) intuitive controls, and 6) exploration interfaces.

Inverse synthesis, also referred to as sound matching, is an example-based automatic synthesizer programming method and has been a major focus of research in this field. The goal of inverse synthesis is to find a parameter setting for a particular synthesizer that matches a target sound as closely as possible. Chapter \ref{chapter:asp-background} reviewed some of the main approaches to inverse synthesis, including genetic algorithms and deep learning. Chapter \ref{chapter:spiegelib} presented an open source software library that was developed as a part of this thesis to encourage shared evaluations and reproducible research \cite{vandewalle2009reproducible}. This library, which was named SpiegeLib, contains all the code necessary to conduct inverse synthesizer experiments using VST software synthesizers and serves as a repository for sharing methods.

Chapter \ref{chapter:inverse_synth_experiment} discussed an inverse synthesizer experiment that was conducted using an open-source emulation of the Yamaha DX7 synthesizer. The goal of this experiment was two-fold: 1) to evaluate existing genetic algorithms (GAs) and deep learning approaches using a benchmark task, and 2) to introduce a novel hybrid approach for inverse synthesis. Results of the evaluation showed that amongst the existing methods, a multi-objective genetic algorithm (MOGA) \cite{tatar2016automatic} was most consistently able to produce high-quality results and outperformed the deep learning methods. Comparison within the deep learning approaches showed that a recurrent neural network architecture proposed by Yee-King \textit{et al.} performed the best \cite{yee2018automatic}. An unexpected result was that all the deep learning approaches performed better using an MFCC audio representation compared to a higher resolution Mel-Spectrogram representation. 

While the MOGA based approach outperformed all the deep learning models, computing a single prediction was significantly more expensive in terms of compute time. A novel hybrid approach was introduced that leveraged the strengths of each method: this approach used deep learning for the initial population generation for a multi-objective genetic algorithm. This approach was able to generate solutions that were as good or better than the original MOGA in 40\% of the time. These results show the potential for combining deep learning with other algorithmic approaches such as MOGAs for automatic synthesizer programming applications.

The results of these experiments also highlighted some of the challenges currently facing inverse synthesis research. The most daunting of these challenges is the complexity of the synthesizer parameter space. %Even in the benchmark task that was used for this experiment, which used a fraction of the total available parameters available, experimental results showed that achieving low auditory error was not dependent on achieving low parameter error. In other words, there are many parameter settings (that are potentially quite different within the parameter space) that produce auditory results that are quite similar. 
Developing a deeper understanding of the relationship between synthesizer parameters and the resulting audio will be critical in advancing research on inverse synthesis.

Motivated by the identified challenges facing automatic synthesizer programming research, a large-scale dataset (synth1B1) and an open-source GPU-enabled modular synthesizer (torchsynth) were developed as a part of this thesis work to support further research in this area. These are presented in chapter \ref{chapter:torchsynth}. Improving the efficacy of deep learning approaches for automatic synthesizer programming is a promising area for future development. The synth1B1 dataset and associated torchsynth synthesizer should allow researchers to more efficiently conduct training and pre-training of deep learning models on synthesizers, and more readily explore the relationship between parameters and the associated auditory output. 

% One of the main questions that arose during the development of synth1B1 was how to sample the parameters of torchsynth to generate one billion diverse synthesizer sounds. The proposed solution used parameter scaling curves for each parameter in torchsynth to skew the distribution of parameter values during sampling. Tuning the hyperparmeters for these curves allowed us to control the distribution of sounds within synth1B1 -- for example, we experimented with selecting hyperparameters to maximize the likelihood that percussion-like sounds were generated. Selecting hyperparmeter values that maximize the likelihood that generated sounds the sound like another synthesizer or sound like a human programmed them is left as an open question for future work.

The final chapter of this thesis, chapter \ref{chapter:synth-explore}, discussed a new automatic synthesizer programming application built on top of the torchsynth synthesizer. This application, called Synth Explorer, provides a visual representation of sounds that are arranged on a two-dimensional interface based on sound similarity. The arrangement and colour of the visualization is controlled by the user, allowing them to construct a visual representation of synthesizer sounds that suites their creative needs. It is an example of an exploration-based interface, and was designed to encourage novice users to explore the auditory space of a synthesizer and begin to learn the relationship between sounds and synthesizer parameters. This development was based on a proposed design framework derived from the fields of creativity support \cite{shneiderman2007creativity} and music interaction \cite{holland2013music}. One of the benefits of exploration-based interfaces is that they engage the user in the process of searching for sounds within a synthesizer, as opposed to taking over the process as is the case with example-based (inverse synthesis) applications. Creating effective and engaging automatic synthesizer programming interfaces will likely benefit from the combination of multiple interaction styles. For example, adding an example-based search option within Synth Explorer, using the methods identified in chapter \ref{chapter:inverse_synth_experiment}, would help support exploration and efficient searching of a synthesizer.

\section{Future Work}
The author hopes that the open-source software and datasets published as a component of this thesis will contribute to further research into automatic synthesizer programming. Clearly, additional work is required to continue to deepen our understanding of the relationship between synthesizer parameters and the resulting audio. The synth1B1 dataset and associated torchsynth synthesizer provide a tool to support research in this direction. However, synth1B1 currently only represents sounds generated using a subtractive synthesis paradigm. Developing more synthesizers in torchsynth using other synthesis methods, such as FM, will help support further research.

One of the limiting factors that was highlighted in chapter \ref{chapter:torchsynth} is the lack of a suitable audio representation that captures the salient perceptual qualities of synthesizer sounds. The development of a perceptually relevant representation of audio, and especially timbre, will help researchers quantify the similarity of synthesizer sounds and further support the development of automated methods of synthesizer programming. Further exploration of the role of audio representations in deep learning approaches will also help to support future development in this area. 

%The unexpected results of the experiment conducted as part of this thesis and described in chapter \ref{chapter:inverse_synth_experiment}, where lower resolution MFCCs performed better than Mel-Spectrograms when used as an input representation, also point to the need for further experiments. 
A current limitation of deep learning approaches for inverse synthesis is the inability to calculate training loss on audio results. This is because there are currently no available methods for computing gradients over traditional software synthesizers, which eliminates them from being used within the training loop of a deep learning algorithm. Instead, researchers must calculate training loss using parameter error. This approach is limited due to the complex nature of the synthesis parameter space, as discussed in chapter \ref{chapter:inverse_synth_experiment}. Developing methods that allow for synthesizers to be included in the training loop of a deep learning algorithm is a promising route for improving the performance of these methods for inverse synthesis. Ram√≠rez \textit{et al.} recently published work that used stochastic gradient estimation to include black-box audio effects within a deep learning model \cite{ramirez2021differentiable}. This approach could likely be adapted to work with synthesizers. Engel \textit{et al.}'s recent work on differentiable digital signal processing (DDSP) provides another possible avenue for including synthesis algorithms within deep learning models \cite{engel2020ddsp}. The torchsynth synthesizer introduced in chapter \ref{chapter:torchsynth} was inspired by this work and future work on that includes updating it to support the computation of gradients over synthesis modules.

Even if the technical hurdles in automatic synthesizer programming can be solved, it will still be necessary to develop more intuitive user interfaces to better assist synthesizer users in achieving their creative goals. The user study conducted by Krekovi\'{c} identified the desire amongst experienced synthesizer users for improved methods for working with their synthesizers \cite{krekovic2019insights}. The responses also suggested that intuitive control interfaces and example-based approaches would be helpful. Determining the best approaches to integrating artificial intelligence and machine learning based techniques with a user interface that supports creativity will require future work.

\section{Final Remarks}
A well designed sound on a synthesizer has the ability to enhance a musical composition and transport a listener to another world. Those who have mastered the art of synthesizer programming have contributed some of the most widely acclaimed musical recordings and film scores; from the best-selling classical compositions of Bach re-imagined by Wendy Carlos in \textit{Switched on Bach} to the \textit{Deep Note} (the "THX sound") designed by Dr. Moorer that played at the beginning of the 1983 premiere of \textit{Star Wars: Episode VI - Return of the Jedi}. With the rise of the personal computer, technology for music and audio production has become increasingly decentralized. Anyone with a laptop or a mobile phone has the power to make sounds with a synthesizer. However, anyone who has tried to program a new sound into a synthesizer will attest to the fact that the usability of these devices has yet to catch up to their availability. The author hopes that the work presented in this thesis well help to contribute to the goal of achieving the best possible mode of automatic synthesizer programming so as to enable users to express themselves more freely with synthesizers and to empower them to create the next great sound.

% \cite{ramirez2021differentiable} -- including VSTis in the training loop and calculating the loss directly on an audio representation.